import tensorflow as tf
import numpy as np
import hashlib
import random
import string
import os
import pandas as pd

# üîπ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
MAX_SEQ_LENGTH = 10
HASH_LENGTH = 64  # SHA-256 —Ö—ç—à –≤ HEX
CHARACTER_SET = string.ascii_letters + string.digits  # "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789"
EMBEDDING_DIM = 512  # –£–≤–µ–ª–∏—á–∏–ª–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
HIDDEN_UNITS = 1024  # –ë–æ–ª—å—à–µ –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ —Å–ª–æ—è—Ö
LEARNING_RATE = 0.0005
BATCH_SIZE = 32

DATASET_FILE = "dataset.csv"
MODEL_PATH = "sha256_decoder.keras"

# üîπ –§—É–Ω–∫—Ü–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
def generate_random_text():
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ª—É—á–∞–π–Ω—É—é —Å—Ç—Ä–æ–∫—É –¥–ª–∏–Ω–æ–π –æ—Ç 1 –¥–æ 10 —Å–∏–º–≤–æ–ª–æ–≤."""
    length = random.randint(1, MAX_SEQ_LENGTH)
    return ''.join(random.choices(CHARACTER_SET, k=length))

def sha256_hash(text):
    """–í—ã—á–∏—Å–ª—è–µ—Ç SHA-256 —Ö—ç—à —Å—Ç—Ä–æ–∫–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –µ–≥–æ –≤ hex-—Ñ–æ—Ä–º–∞—Ç–µ."""
    return hashlib.sha256(text.encode()).hexdigest()

def generate_dataset(size):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ —Å–ª—É—á–∞–π–Ω—ã—Ö —Å—Ç—Ä–æ–∫ –∏ –∏—Ö SHA-256 —Ö—ç—à–µ–π."""
    data = [(generate_random_text(), None) for _ in range(size)]
    return [(text, sha256_hash(text)) for text, _ in data]

# üîπ –†–∞–±–æ—Ç–∞ —Å —Ñ–∞–π–ª–æ–º –¥–∞—Ç–∞—Å–µ—Ç–∞
def load_dataset():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ —Ñ–∞–π–ª–∞, –µ—Å–ª–∏ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç."""
    if os.path.exists(DATASET_FILE):
        df = pd.read_csv(DATASET_FILE, header=None)
        return list(zip(df[0], df[1]))
    return []

def save_dataset(dataset):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –≤ CSV."""
    df = pd.DataFrame(dataset)
    df.to_csv(DATASET_FILE, index=False, header=False)

# üîπ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
def text_to_one_hot(text, max_length, char_to_idx):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –≤ one-hot –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ."""
    seq = [char_to_idx.get(c, 0) for c in text]
    return seq + [0] * (max_length - len(seq))  # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ –¥–æ max_length

def hash_to_vector(hash_str):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç SHA-256 —Ö—ç—à –≤ —á–∏—Å–ª–æ–≤–æ–π –≤–µ–∫—Ç–æ—Ä (–ø–æ ASCII-–∫–æ–¥–∞–º)."""
    return [ord(c) for c in hash_str]

def prepare_data(dataset, char_to_idx):
    """–ì–æ—Ç–æ–≤–∏—Ç –≤—Ö–æ–¥–Ω—ã–µ –∏ –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è."""
    x_data = np.array([hash_to_vector(h) for _, h in dataset])
    y_data = np.array([text_to_one_hot(t, MAX_SEQ_LENGTH, char_to_idx) for t, _ in dataset])
    return x_data, y_data

# üîπ –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
def create_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(HASH_LENGTH,)),

        # –í—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤
        tf.keras.layers.Embedding(input_dim=256, output_dim=EMBEDDING_DIM),

        # –ú–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–π LSTM
        tf.keras.layers.LSTM(HIDDEN_UNITS, return_sequences=True),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.LSTM(HIDDEN_UNITS, return_sequences=True),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.LSTM(HIDDEN_UNITS),
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        tf.keras.layers.Dense(HIDDEN_UNITS, activation="relu"),
        tf.keras.layers.Dense(HIDDEN_UNITS, activation="relu"),

        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        tf.keras.layers.Dense(MAX_SEQ_LENGTH * len(CHARACTER_SET), activation="softmax"),
        tf.keras.layers.Reshape((MAX_SEQ_LENGTH, len(CHARACTER_SET)))
    ])
    
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
                  loss="categorical_crossentropy",
                  metrics=["accuracy"])
    return model

# üîπ –ó–∞–≥—Ä—É–∑–∫–∞ / —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
def load_model():
    if os.path.exists(MODEL_PATH):
        print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å...")
        return tf.keras.models.load_model(MODEL_PATH)
    print("üÜï –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å...")
    return create_model()

def save_model(model):
    model.save(MODEL_PATH)
    print("üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")

# üîπ –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
if __name__ == "__main__":
    model = load_model()

    char_to_idx = {char: idx for idx, char in enumerate(CHARACTER_SET)}
    idx_to_char = {idx: char for char, idx in char_to_idx.items()}

    dataset = load_dataset()
    dataset_size = len(dataset)  # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞
    print(f"üìÇ –ù–∞–π–¥–µ–Ω–æ {dataset_size} –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ.")

    for i in range(9999):  # –î–æ–ª–≥–æ–µ –æ–±—É—á–µ–Ω–∏–µ
        new_samples = 25  # –î–æ–±–∞–≤–ª—è–µ–º 25 –Ω–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
        print(f"\nüîπ –ò—Ç–µ—Ä–∞—Ü–∏—è {i+1}: –¥–æ–±–∞–≤–ª—è–µ–º {new_samples} –ø—Ä–∏–º–µ—Ä–æ–≤...")
        
        new_data = generate_dataset(new_samples)
        dataset.extend(new_data)
        save_dataset(dataset)  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç

        x_train, y_train = prepare_data(dataset, char_to_idx)

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ y_train –≤ one-hot –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
        y_train_one_hot = tf.keras.utils.to_categorical(y_train, num_classes=len(CHARACTER_SET))

        print(f"üöÄ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ {len(dataset)} –ø—Ä–∏–º–µ—Ä–∞—Ö...")
        model.fit(x_train, y_train_one_hot, epochs=25, batch_size=BATCH_SIZE)

        save_model(model)  # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏

    print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
